{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5obvCKviaGy"
      },
      "source": [
        "# Data-Cleaning Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGvTOt9IVRSx"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQJay43N2zwa",
        "outputId": "ae170c65-f7b0-451a-ded5-a072295f1d4a"
      },
      "source": [
        "!pip install langdetect\n",
        "!pip install -U textblob\n",
        "!pip install -U textblob-fr\n",
        "!python -m textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
            "Requirement already satisfied: textblob-fr in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: textblob>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from textblob-fr) (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob>=0.8.0->textblob-fr) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob>=0.8.0->textblob-fr) (1.15.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zOI9qt5w-Fg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from langdetect import DetectorFactory, detect, detect_langs\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import datetime as datetime\n",
        "from textblob import TextBlob\n",
        "from textblob_fr import PatternTagger, PatternAnalyzer\n",
        "\n",
        "text = u\"Quelle belle matin√©e\"\n",
        "blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlGbfbMv03k9"
      },
      "source": [
        "df = pd.read_csv('/content/Barkha_E2.csv')\n",
        "df=df.set_axis([*df.columns[:-1], 'Reply'], axis=1, inplace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "egPS47Xm081h",
        "outputId": "b2f4e1bc-54ef-4120-a335-3e5ea0e0c8d1"
      },
      "source": [
        "df.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Reply_Count</th>\n",
              "      <th>Time</th>\n",
              "      <th>date</th>\n",
              "      <th>hour</th>\n",
              "      <th>Reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>Kuruangha Yamii</td>\n",
              "      <td>ill definitely fall in love with ayush if i we...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-06T19:34:27Z</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>roopesh Raj</td>\n",
              "      <td>barkha mam and ayush sir you both look very cu...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-05T10:51:30Z</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>Garima Parajuli</td>\n",
              "      <td>ayush and barkha onscreen chemistry is fire</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-11-05T05:29:18Z</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Robin Lalwani</td>\n",
              "      <td>aayush and barkha you look soooo good together...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-12-17T19:42:57Z</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>Soumya</td>\n",
              "      <td>can someone tell me the name of the guy who is...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-11-06T09:37:54Z</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Name  ... Reply\n",
              "110  Kuruangha Yamii  ...    11\n",
              "250      roopesh Raj  ...    11\n",
              "307  Garima Parajuli  ...    11\n",
              "37     Robin Lalwani  ...    12\n",
              "134           Soumya  ...    11\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY41Ufq4VRS9"
      },
      "source": [
        "# preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSlOcbwdYSW2"
      },
      "source": [
        "df_copy = df.copy()\n",
        "df_copy['Comment'] = [BeautifulSoup(text).get_text() for text in df_copy['Comment']]#remove HTML tags\n",
        "df_copy['Comment'] = df_copy['Comment'].apply(lambda x: re.split(' \"https:\\/\\/.*', str(x))[0])#remove HTTP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axZmvJPD0_AM"
      },
      "source": [
        "##### Clean data creation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf6KFKcz1GJo"
      },
      "source": [
        "\n",
        "\n",
        "# lists for the data columns \n",
        "Name=[]\n",
        "Comment=[]\n",
        "Time=[]\n",
        "Likes=[]\n",
        "Reply=[]\n",
        "Time=[]\n",
        "# list for checking error in detect Library\n",
        "done=[]\n",
        "Notedone=[]\n",
        "#loop for the new data\n",
        "for i in range(len(df_copy)):\n",
        "    try:\n",
        "        if detect(df_copy['Comment'][i]) == 'en':\n",
        "            Comment.append(df_copy['Comment'][i])\n",
        "            done.append(i)#check\n",
        "            #others appended\n",
        "            Name.append(df_copy['Name'][i])\n",
        "            Time.append(df_copy['Time'][i])\n",
        "            Likes.append(df_copy['Likes'][i])\n",
        "            Reply.append(df_copy['Reply'][i])\n",
        "            Time.append(df_copy['Time'][i])\n",
        "    except:\n",
        "        Notedone.append(i)#check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J-iEfX-VRTA",
        "outputId": "3371d151-7787-4490-b343-5131bace943d"
      },
      "source": [
        "print(len(Name),len(Comment),len(Time),len(Likes),len(Reply))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "531 531 1062 531 531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k63h9M6YVRTB",
        "outputId": "4a0ee77f-ff80-45bb-bdcf-c29d965ce621"
      },
      "source": [
        "print(len(Notedone),'text languages is not getting detected and gave and error')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 text languages is not getting detected and gave and error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1XGFpSX1PGL"
      },
      "source": [
        "df_cl = pd.DataFrame(list(zip(Name,Comment,Time,Likes,Reply)),columns =['Names','Comment','Time','Likes','Reply_count'])\n",
        "#df_cl.to_csv('clean_data.csv',index=False)#save\n",
        "#df_cl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UHnB6EEYs8Q"
      },
      "source": [
        "temp= pd.DataFrame(list(Comment),columns=['Comment'])\n",
        "temp1= pd.DataFrame(list(Likes),columns=['Likes'])\n",
        "temp2= pd.DataFrame(list(Name),columns=['Name'])\n",
        "temp3= pd.DataFrame(list(Reply),columns=['Reply_Count'])\n",
        "temp4 = pd.DataFrame(list(Time),columns=['Time'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVRFR9LGYOZm"
      },
      "source": [
        "temp['Comment'] = temp.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reX4sZEZWb1G"
      },
      "source": [
        "#removing special characters from comments\n",
        "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
        "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
        "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
        "              \"`\",\"{\",\"|\",\"}\",\"~\",\"‚Äì\"]\n",
        "for char in spec_chars:\n",
        "    temp['Comment'] = temp['Comment'].str.replace(char, ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YN4dPlAZbGF"
      },
      "source": [
        "temp['Comment'] = temp['Comment'].str.split().str.join(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g6nmrFCZoZS"
      },
      "source": [
        "#removing emojis\n",
        "temp2['Name'] = temp2.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mSuPJYdaaBX"
      },
      "source": [
        "#removing special characters from name\n",
        "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
        "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
        "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
        "              \"`\",\"{\",\"|\",\"}\",\"~\",\"‚Äì\"]\n",
        "for char in spec_chars:\n",
        "    temp2['Name'] = temp2['Name'].str.replace(char, ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3CQjmgJi5kL"
      },
      "source": [
        "### Concating data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLYAB79BbcVX"
      },
      "source": [
        "result = pd.concat([temp2, temp], axis=1, join='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3wUei2vcHB7"
      },
      "source": [
        "result = pd.concat([result, temp1], axis=1, join='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPzvmyzLcMWa"
      },
      "source": [
        "result = pd.concat([result, temp3], axis=1, join='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV8X-aqVcQGI"
      },
      "source": [
        "result = pd.concat([result, temp4], axis=1, join='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yK5Zi8vAwFo"
      },
      "source": [
        "from datetime import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_8p0f9VAPkv"
      },
      "source": [
        "#Extracting month, date and hour from time column\n",
        "date = []\n",
        "hour = []\n",
        "month = []\n",
        "for i in range(len(result)):\n",
        "    d1 = datetime.fromisoformat(result['Time'][i][:-1])\n",
        "    h = d1.strftime('%H')\n",
        "    d = d1.strftime('%d')\n",
        "    m = d1.strftime('%m')\n",
        "    date.append(d)\n",
        "    hour.append(h)\n",
        "    month.append(m)\n",
        "\n",
        "result['date']=date\n",
        "result['hour']=hour\n",
        "result['month']=month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXdpNYo-cRUS"
      },
      "source": [
        "#lowering the strings\n",
        "result['Comment'] = result['Comment'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS9FVmIkdD1M",
        "outputId": "9a52bdb0-2ab8-49ab-caa3-78095f7b075f"
      },
      "source": [
        "result.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Reply_Count</th>\n",
              "      <th>Time</th>\n",
              "      <th>date</th>\n",
              "      <th>hour</th>\n",
              "      <th>month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dice Media</td>\n",
              "      <td>make your mornings exciting healthy with so go...</td>\n",
              "      <td>2907</td>\n",
              "      <td>285</td>\n",
              "      <td>2020-11-24T08:37:17Z</td>\n",
              "      <td>24</td>\n",
              "      <td>08</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Squad x 69 Esports</td>\n",
              "      <td>will wait for season 3 barkha singh and ayush ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-24T08:37:17Z</td>\n",
              "      <td>24</td>\n",
              "      <td>08</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Brainable Brains</td>\n",
              "      <td>such a great series i enjoyed watching from be...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-09-15T17:28:23Z</td>\n",
              "      <td>15</td>\n",
              "      <td>17</td>\n",
              "      <td>09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Atul Sen</td>\n",
              "      <td>waiting for 3rd session</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-09-15T17:28:23Z</td>\n",
              "      <td>15</td>\n",
              "      <td>17</td>\n",
              "      <td>09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sijal Soni</td>\n",
              "      <td>i am so glad to watch this wed session and all...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-09-14T20:37:38Z</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Name  ... month\n",
              "0          Dice Media  ...    11\n",
              "1  Squad x 69 Esports  ...    11\n",
              "2    Brainable Brains  ...    09\n",
              "3            Atul Sen  ...    09\n",
              "4          Sijal Soni  ...    09\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_0ukUnHdF8e"
      },
      "source": [
        "result.to_csv('PFA S2 E6_cleaned.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut5RJOvrCBm7"
      },
      "source": [
        "# Sentimental Scoring methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad02XrMqjLJw"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-yHOVleFTOp",
        "outputId": "c4a25bec-fc4d-430e-9a18-7be607d1270b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9fr0W8fiO6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5a02d5-a4aa-44d5-f96b-89c39caf60ab"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7E7kt9cFkHy"
      },
      "source": [
        "#tokenizing the comments\n",
        "df['Tokenize']= df['Comment'].apply(word_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flAnrpzMFokb"
      },
      "source": [
        "#creating stem of comments\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHHVnV80F4b6"
      },
      "source": [
        "df['stemmed'] = df['Tokenize'].apply(lambda x: [stemmer.stem(y) for y in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8wXjCb0HFVl"
      },
      "source": [
        "#Creating sentiment scores positive, negative, neutral and compound\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "df['compound'] = [analyzer.polarity_scores(x)['compound'] for x in df['Comment']]\n",
        "df['neg'] = [analyzer.polarity_scores(x)['neg'] for x in df['Comment']]\n",
        "df['neu'] = [analyzer.polarity_scores(x)['neu'] for x in df['Comment']]\n",
        "df['pos'] = [analyzer.polarity_scores(x)['pos'] for x in df['Comment']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sLqmAa6Jnqn"
      },
      "source": [
        "#Calculating polarity and subjectivity\n",
        "df[['polarity', 'subjectivity']] = df['Comment'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwnxwtr2szHU"
      },
      "source": [
        "### Empath score \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdI1F4q9O_YZ",
        "outputId": "2be04187-b98c-4c81-bc91-8b7067bffa58"
      },
      "source": [
        "!pip install empath"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: empath in /usr/local/lib/python3.7/dist-packages (0.89)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from empath) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (2021.5.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m46TTr6C1Ml"
      },
      "source": [
        "#function for creating dictionary\n",
        "def returnSum(dict): \n",
        "    sum = 0\n",
        "    for i in dict: \n",
        "        sum = sum + dict[i]  \n",
        "    return sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Myqf96CrPUAi"
      },
      "source": [
        "#calculating empath score\n",
        "from empath import Empath\n",
        "lexicon = Empath()\n",
        "l = []\n",
        "m = []\n",
        "for i in range(len(df)):\n",
        "  lex= lexicon.analyze(df['Comment'][i], normalize=True)\n",
        "  emp = empath=lexicon.analyze(df['Comment'][i], normalize=True)\n",
        "  empath_score= returnSum(empath) \n",
        "  l.append(empath_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tEWvTvxgHUT"
      },
      "source": [
        "l = pd.DataFrame(list(l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQIA00xkn7RV",
        "outputId": "b707bba2-5f8c-404e-d374-b48e4558f438"
      },
      "source": [
        "l.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 206 entries, 0 to 205\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       206 non-null    float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 1.7 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5rrabsyotRq"
      },
      "source": [
        "l.rename(columns = {0:'empath_score'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xZUTO-XloytK",
        "outputId": "f74d2d43-a70f-47a8-e3f0-393210216921"
      },
      "source": [
        "df = pd.concat([df, l], axis=1, join='inner')\n",
        "#display(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Reply_Count</th>\n",
              "      <th>Time</th>\n",
              "      <th>date</th>\n",
              "      <th>hour</th>\n",
              "      <th>month</th>\n",
              "      <th>Tokenize</th>\n",
              "      <th>stemmed</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>empath_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CR MOTIONS</td>\n",
              "      <td>here s a teaser of ayush mehra and barkha sing...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-08-29T12:40:33Z</td>\n",
              "      <td>29</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>[here, s, a, teaser, of, ayush, mehra, and, ba...</td>\n",
              "      <td>[here, s, a, teaser, of, ayush, mehra, and, ba...</td>\n",
              "      <td>0.4215</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.726</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.038462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rishika Chakraborty</td>\n",
              "      <td>barkha and aayush are looking so perfect together</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-08-09T12:58:28Z</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>[barkha, and, aayush, are, looking, so, perfec...</td>\n",
              "      <td>[barkha, and, aayush, are, look, so, perfect, ...</td>\n",
              "      <td>0.6948</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.596</td>\n",
              "      <td>0.404</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aman Sharma</td>\n",
              "      <td>put the entire show aside the long strand of h...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-04-02T15:02:46Z</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>[put, the, entire, show, aside, the, long, str...</td>\n",
              "      <td>[put, the, entir, show, asid, the, long, stran...</td>\n",
              "      <td>0.4404</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.879</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Abdul Moiz</td>\n",
              "      <td>barkha is beautiful and ayush is handsomewhat ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-04-01T22:19:01Z</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>4</td>\n",
              "      <td>[barkha, is, beautiful, and, ayush, is, handso...</td>\n",
              "      <td>[barkha, is, beauti, and, ayush, is, handsomew...</td>\n",
              "      <td>0.8402</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.471</td>\n",
              "      <td>0.825000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>usic Addiction</td>\n",
              "      <td>barkha and ayush look s super cute as they are...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-03-29T00:50:22Z</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[barkha, and, ayush, look, s, super, cute, as,...</td>\n",
              "      <td>[barkha, and, ayush, look, s, super, cute, as,...</td>\n",
              "      <td>0.9719</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.395</td>\n",
              "      <td>0.605</td>\n",
              "      <td>0.568056</td>\n",
              "      <td>0.540278</td>\n",
              "      <td>0.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>Avani Savadatti</td>\n",
              "      <td>i am very exited to see this viedo yay i am bi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-09T12:51:54Z</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>11</td>\n",
              "      <td>[i, am, very, exited, to, see, this, viedo, ya...</td>\n",
              "      <td>[i, am, veri, exit, to, see, this, viedo, yay,...</td>\n",
              "      <td>0.6908</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.695</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.058824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>Khushi</td>\n",
              "      <td>am sooo damn excited for this i waited for thi...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-09T12:45:26Z</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>11</td>\n",
              "      <td>[am, sooo, damn, excited, for, this, i, waited...</td>\n",
              "      <td>[am, sooo, damn, excit, for, this, i, wait, fo...</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.751</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.231481</td>\n",
              "      <td>0.440741</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>Bhabani Kalita</td>\n",
              "      <td>ayush and barkha is love</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-09T12:39:54Z</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>11</td>\n",
              "      <td>[ayush, and, barkha, is, love]</td>\n",
              "      <td>[ayush, and, barkha, is, love]</td>\n",
              "      <td>0.6369</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.488</td>\n",
              "      <td>0.512</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>Palak Negi</td>\n",
              "      <td>barkha op ayush op dice media op</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-09T12:39:54Z</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>11</td>\n",
              "      <td>[barkha, op, ayush, op, dice, media, op]</td>\n",
              "      <td>[barkha, op, ayush, op, dice, media, op]</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>its  aman</td>\n",
              "      <td>really really waiting for this one of my favou...</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-09T12:30:15Z</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>11</td>\n",
              "      <td>[really, really, waiting, for, this, one, of, ...</td>\n",
              "      <td>[realli, realli, wait, for, this, one, of, my,...</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.055556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>206 rows √ó 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Name  ... empath_score\n",
              "0             CR MOTIONS  ...     0.038462\n",
              "1    Rishika Chakraborty  ...     0.500000\n",
              "2            Aman Sharma  ...     0.375000\n",
              "3             Abdul Moiz  ...     0.416667\n",
              "4         usic Addiction  ...     0.480000\n",
              "..                   ...  ...          ...\n",
              "201      Avani Savadatti  ...     0.058824\n",
              "202               Khushi  ...     0.200000\n",
              "203       Bhabani Kalita  ...     1.000000\n",
              "204           Palak Negi  ...     0.000000\n",
              "205            its  aman  ...     0.055556\n",
              "\n",
              "[206 rows x 17 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR0fj1Kd7XyM"
      },
      "source": [
        "def maximum(a, b, c):\n",
        "  \n",
        "    if (a >= b) and (a >= c):\n",
        "        largest = 'neg'\n",
        "  \n",
        "    elif (b >= a) and (b >= c):\n",
        "        largest = 'neu'\n",
        "    else:\n",
        "        largest = 'pos'\n",
        "          \n",
        "    return largest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2VJdnPe7Y3M"
      },
      "source": [
        "l=[]\n",
        "for i in range(len(result)):\n",
        "    x=maximum(df['neg'][i],df['neu'][i],df['pos'][i])\n",
        "    l.append(x)\n",
        "\n",
        "df['sentiment_status']=l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P__ghqaqChp"
      },
      "source": [
        "#Final dataset cleaned and scored\n",
        "df.to_csv(\"Barkha E3 score.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}